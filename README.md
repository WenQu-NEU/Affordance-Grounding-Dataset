# Affordance-Grounding-Dataset(Coming soon)

UMD VL and IIT-AFF VL Dataset for Affordance Grounding
This link provides two visual language dataset (UMD VL and IIT-AFF VL) for affordance grounding with paired data <T, I, M>, comprising natural language instruction T, corresponding RGB image I, and pixel-level part affordance mask the M.

UMD VL dataset:
1. We selected 7,070 images from the original UMD dataset.
2. For each image, its corresponding language instruction and affordance mask have the same name with the image. 
3. The dataset is splited into training, testing and validation following the train.txt, test.txt and validate.txt files.

ITT-AFF VL dataset:
1. We used the images from IIT-AFF dataset, which has 8,835 real-world images under cluttered scenes. 
2. For each image, its corresponding language instruction and affordance mask have the same name with the image.
3. The dataset is splited into training, testing and validation following the train.txt, test.txt and validate.txt files.


